{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgJjPc7qwYgX"
   },
   "source": [
    "# Suppose that we have just been employed as a data scientist to help improve online customer support at a leading airline company. In our first meeting, we have been tasked with providing a summary of the online support required on the Twitter social media platform. The manager of the help desk support team in particular would like a summary of main Twitter topics that would require a reply via Twitter. Even though tweets are short in length, the dataset would be very big and difficult to manually read! You require a Natural Language Processing (NLP) technique that can help you meet the deadline and that can be done using topic modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP3Xm8OqyGJo"
   },
   "source": [
    "# There are several topic modelling algorithms that can be used to perform assignment of different topics and their weightages to the documents (in this case each individual tweet is a document), called topic vector of each document or semantic vector of each document whose dimensionality will be equal to the number of topics which you want to assign to each document. We will be using different topic modelling algorithms on different datasets to convert tweets to semantic or topic vectors, such as NMF (Non Negative Matrix Factorization), LDA (Latent Dirichilet Allocation) and BERT (Bidirectional Encoder Representations from Transformers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HsSssyqxMxR"
   },
   "source": [
    "# Topic modeling algorithms provide a way to find the main topics that are discussed in a text corpus. Topic modeling is an unsupervised technique that only requires the text corpus and is able to derive the topics without any manually labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihivoFu811K4"
   },
   "source": [
    "### Let's first download the dataset from Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unJQb5VQ2GCo"
   },
   "source": [
    "### To know more about the dataset, we can navigate to the folowing url: \n",
    "https://www.kaggle.com/crowdflower/twitter-airline-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2SBtwZN2kF4",
    "outputId": "b352ce6b-2b5c-4b05-ede4-29a23768afed"
   },
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEVT-xe4223q",
    "outputId": "443f3f73-8238-4457-f8c7-87b7d77aa10a"
   },
   "outputs": [],
   "source": [
    "! unzip ./archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyObIK0y4IXj",
    "outputId": "8eeec19a-cbfe-4645-c001-1180745f818a"
   },
   "outputs": [],
   "source": [
    "! pip install octis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vecaNjZh4W8W",
    "outputId": "340e6a3d-9a41-4545-c642-2da7cf6d7907"
   },
   "outputs": [],
   "source": [
    "! pip install delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daBQ62V44sbB",
    "outputId": "0c4584e7-5689-4b66-f48d-91053f0b082a"
   },
   "outputs": [],
   "source": [
    "! pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qJ5hYvL49qa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import preprocessor as tweet_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7J-bto_6JBT"
   },
   "source": [
    "###Although, we have been using a combination of NLTK as well as Spacy till now but we are going to use here only Spacy to perform text preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybZpSSlz5xe-"
   },
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en')\n",
    "\n",
    "# Add custom stopwords to Spacy\n",
    "spacy_nlp.Defaults.stop_words |= {\"...\",\"---\",\"makemeastopword\"}\n",
    "\n",
    "# Create a method called spacy_custom_tokenizer that performs custom tokenization using Spacy\n",
    "def tokenizer(single_tweet):\n",
    "\n",
    "    #Let's clean the tweet using tweet_processor\n",
    "    single_cleaned_tweet = tweet_processor.clean(single_tweet)\n",
    "\n",
    "    spacy_tweet = spacy_nlp(single_cleaned_tweet)\n",
    "\n",
    "    # Remove stop words, punctuation and numbers\n",
    "    # Return he lemma of the word as lowercase and remove extra spaces\n",
    "    tokens = [word.lemma_.lower().strip() for word in spacy_tweet if (not word.is_stop and not word.is_punct and not word.like_num)]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LaVy2xJ7Nbw"
   },
   "source": [
    "### Let's read the Twitter Airline Sentiment dataset downloaded from Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5HEDc0_6Ct_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rdcn-Ov47mdE"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5MQtJ147qMm"
   },
   "outputs": [],
   "source": [
    "airline_data = data[data['airline']=='US Airways']\n",
    "\n",
    "airline_tweets = airline_data['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "id": "XO2tM8DX8KUX",
    "outputId": "a601bd9f-8fca-48c2-fe5b-09b42c875ee7"
   },
   "outputs": [],
   "source": [
    "airline_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv_o-4JiClhr"
   },
   "source": [
    "### Non Negative Matrix Factrization (NMF) is a Matrix Factroization technique similar to Singular Value Decomposition (SVD). In SVD, a matrix gets factorized into three matrices (U, $\\lambda$ and V), whereas in NMF, a matrix gets factorized into two matrices, that is:\n",
    "\n",
    "### \\begin{equation}\n",
    "X = H\\quad\\bullet\\quad W\n",
    "\\end{equation}\n",
    "\n",
    "### The word \"Non Negative\" here actually means that all the entries in the matrices $H$ and $W$ have positive or non negative entries. \n",
    "\n",
    "### Where, $X$ = Data Matrix of shape (Number of tweets, Number of words in vocabulary). It can be any kind of matrix such as TF, TF-IDF, Co-occurrence, PMI or PPMI matrix. \n",
    "\n",
    "### $H$ = Document-Topic Matrix of shape (Number of tweets or documents, Number of topics which needs to be selected by us). Each row vector in this matrix is a topic vector representation of each tweet or document, just like TF-IDF is also a vector representation of a tweet or document. The difference is that Topic vector representation is semantic in nature and is more meaningful whereas TF-IDF is simply frequency based and not meaningful.\n",
    "\n",
    "### $W$ = Topic-Vocabulary matrix of shape (Number of topics, Number of words in vocabulary). Each column vector in this matrix is a topic or semantic vector of a word present in a vocabulary. \n",
    "\n",
    "### The number of topics (that is the dimensionality of topic vector) needs to be selected by us. More the dimensionality of topic vector, more accurate will be the semantic representation of a tweet (document) or a word in a vocabulary. The dimensionality of topic vector here is acting like a resolution of semantics behind a document or a word. More the dimensionality, more will be the resolution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O2eBuyVL3iI"
   },
   "source": [
    "### Let's format the calculated matrices in the form which is acceptable for OCTIS library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kiagmtxv8QYK"
   },
   "outputs": [],
   "source": [
    "def format_NMF_output(nmf_H, nmf_W, vocab, number_top_words):\n",
    "  topics = []\n",
    "  for topic_idx, topic in enumerate(nmf_H):\n",
    "    word_list = [vocab[i] for i in topic.argsort()[:-(number_top_words + 1):-1]]\n",
    "    topics.append(word_list)\n",
    "\n",
    "  octis_topic_dict = {}\n",
    "  octis_topic_dict[\"topic-word-matrix\"] = nmf_H\n",
    "  octis_topic_dict[\"topics\"] = topics\n",
    "  octis_topic_dict[\"topic-document-matrix\"] = np.array(nmf_W).transpose()\n",
    "  return octis_topic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIMu5N1IQzpw"
   },
   "outputs": [],
   "source": [
    "def prep_dataset_for_octis(tweets):\n",
    "  dataset_for_octis = []\n",
    "  #Add your code here\n",
    "  return dataset_for_octis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkviLYZNRy6S"
   },
   "outputs": [],
   "source": [
    "number_topics_list = [64, 128, 256] #As already told that the number of topics or the dimensionality needs to be selected by us and therefore we will be\n",
    "#evaluating the quality of topic vectors for different dimensionalities starting from 64 dimensional to 512 dimensional\n",
    "number_top_words = 100 #Set number of top words - will be used by Diversity and display_topics() method\n",
    "number_top_documents = 100 #Set number of top documents - will be used by the display_topics() method\n",
    "\n",
    "additional_stop_words = ['flight', '..', '...', 'help', 'thank', 'thanks','great', 'need', 'response', 'today']\n",
    "stop_words = list(STOP_WORDS) + additional_stop_words\n",
    "\n",
    "npmi = Coherence(texts=prep_dataset_for_octis(airline_tweets), topk=number_top_words, measure='c_npmi')\n",
    "topic_diversity = TopicDiversity(topk=number_top_words)\n",
    "\n",
    "max_iter = 500 #We will talk about this later what is this max_iter\n",
    "\n",
    "#Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv_XV6Z_fQSW"
   },
   "source": [
    "### Let's now apply NMF on our term-document matrix, X for different dimensionalities of topic vectors (64, 128, 256). It means that in the matrix factroization:\n",
    "\n",
    "### \\begin{equation}\n",
    "X = H \\quad \\bullet \\quad W\n",
    "\\end{equation}\n",
    "\n",
    "### For different dimensionalities of topic vectors (64, 128, 256, 512), the matrices, $H$ and $W$ will be of shape:\n",
    "\n",
    "### $H$ : (Number of Documents, 64), $W$ : $(64, 2000)$\n",
    "### $H$ : (Number of Documents, 128), $W$ : $(128, 2000)$\n",
    "### $H$ : (Number of Documents, 256), $W$ : $(256, 2000)$\n",
    "\n",
    "### So, now the question arrises is that how NMF finds out these two matrices for different dimensionalities of topic vectors. Bascially, NMF tries to find out two non negative matrices, $H$ and $W$ such that:\n",
    "\n",
    "### \\begin{equation}\n",
    "||X - H\\bullet W||_2\n",
    "\\end{equation}\n",
    "\n",
    "### gets minimized. And these two matrices are first randomly initialized and then updated through gradient descent algorithm where the gradients involved in gradient descent algorithm will be the gradients of $||X - H\\bullet W||_2$ with respect to matrices $H$ and $W$. Now, we have selected the total number of iterations for which the gradient descent algorithm will run as 500 iterations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ZWxLI3Pek-m",
    "outputId": "0dcf545c-6034-4c88-89fb-499328b21950"
   },
   "outputs": [],
   "source": [
    "reconstruction_error_list = []\n",
    "number_of_iterations_list = []\n",
    "npmi_coherence_score_list = []\n",
    "diversity_score_list = []\n",
    "\n",
    "for number_topics in number_topics_list:\n",
    "\n",
    "  curent_number_topics = number_topics\n",
    "  nmf_model = NMF(n_components=curent_number_topics, init='nndsvd', max_iter=max_iter).fit(term_document_matrix)\n",
    "  nmf_W = nmf_model.transform(term_document_matrix)\n",
    "  nmf_H = nmf_model.components_\n",
    "\n",
    "  reconstruction_error_list.append(nmf_model.reconstruction_err_)\n",
    "  number_of_iterations_list.append(nmf_model.n_iter_)\n",
    "\n",
    "  octis_topic_dict = format_NMF_output(nmf_H, nmf_W, vocab, number_top_words)\n",
    "\n",
    "  npmi_coherence_score_list.append(npmi.score(octis_topic_dict))\n",
    "  diversity_score_list.append(topic_diversity.score(octis_topic_dict))\n",
    "\n",
    "\n",
    "print(\"reconstruction_error_list\", reconstruction_error_list)\n",
    "print(\"number_of_iterations_list\", number_of_iterations_list)\n",
    "print(\"npmi_coherence_score_list\", npmi_coherence_score_list)\n",
    "print(\"diversity_score_list\", diversity_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19NlnsweoAso"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Topic Modelling using NMF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
